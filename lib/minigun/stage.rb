# frozen_string_literal: true

require 'securerandom'

module Minigun
  # Unified context for all stage execution (producers and workers)
  StageContext = Struct.new(
    # Common to all stages
    :stage,
    :dag,
    :runtime_edges,
    :stage_stats,
    # Worker-specific (nil/empty for producers)
    :worker,
    :input_queue,
    :sources_expected,
    :sources_done,
    keyword_init: true
  ) do
    # Convenience method to access executor through worker
    def executor
      worker&.executor
    end

    # Convenience method for stage name (delegates to stage object)
    def stage_name
      stage&.name
    end

    def pipeline
      stage&.pipeline
    end

    def root_pipeline
      pipeline&.root_pipeline
    end
  end

  # Base class for all execution units (stages and pipelines)
  # Implements the Composite pattern where Pipeline is a composite Stage
  # Also handles loop-based stages (stages that manage their own input loop)
  class Stage
    attr_reader :pipeline, :name, :options, :block

    # Positional constructor: Stage.new(name, pipeline, block, options)
    def initialize(name, pipeline, block = nil, options = {})
      @name = name
      @pipeline = pipeline
      @block = block
      @options = options

      # Auto-generate name if not provided (for unnamed stages)
      # Use "_" prefix + 8 char random hex
      # TODO: Convert to base62
      @name = :"_#{SecureRandom.hex(4)}" if @name.nil?

      # Register stage with the task's stage_registry (if available)
      task&.stage_registry&.register(@pipeline, self)
    end

    def task
      @pipeline.task
    end

    def root_pipeline
      @pipeline.root_pipeline
    end

    # Get the queue size for this stage
    # Returns nil for unbounded queues (0, Float::INFINITY, nil)
    # Returns integer for bounded queues (SizedQueue)
    def queue_size
      size = @options[:queue_size]

      # Use global default if not specified
      size = Minigun.default_queue_size if size.nil?

      # Check for unbounded indicators
      return nil if [0, Float::INFINITY, false].include?(size)

      size.to_i
    end

    # Execute the stage with the given context
    # For loop-based stages, this receives input_queue and output_queue
    def execute(context, input_queue, output_queue, _stage_stats)
      if @block
        context.instance_exec(input_queue, output_queue, &@block)
      elsif respond_to?(:call)
        call_with_arity(input_queue, output_queue, &output_queue.to_proc)
      end
    end

    # Run the stage execution
    # Loop-based stages manage their own input loop
    def run_stage(stage_ctx)
      # Create wrapped queues
      input_queue = create_input_queue(stage_ctx) # TODO: move to worker?
      output_queue = create_output_queue(stage_ctx)

      # Execute with both queues (block manages its own loop)
      context = stage_ctx.root_pipeline.context
      execute(context, input_queue, output_queue, stage_ctx.stage_stats)
    ensure
      send_end_signals(stage_ctx)
    end

    # Get execution context configuration for this stage
    def execution_context
      @options[:_execution_context]
    end

    # Hash representation (for test compatibility)
    def to_h
      hash = { name: @name, options: @options }
      hash[:block] = @block if @block
      hash
    end

    # Hash-like access (for test compatibility)
    def [](key)
      case key
      when :name then @name
      when :options then @options
      when :block then @block
      end
    end

    # Type name for logging purposes
    def log_type
      'Worker'
    end

    # Execution strategy: :autonomous, :streaming, or :composite
    def run_mode
      :streaming # Default: process stream of items in worker loop
    end

    def to_s
      "#{self.class.name}(#{name})"
    end

    def inspect
      to_s
    end

    private

    # Create wrapped input queue for this stage
    def create_input_queue(stage_ctx)
      InputQueue.new(
        stage_ctx.input_queue,
        stage_ctx.stage,
        stage_ctx.sources_expected,
        stage_stats: stage_ctx.stage_stats
      )
    end

    # Create wrapped output queue for this stage
    def create_output_queue(stage_ctx)
      # DAG and queues now use Stage objects
      downstream = stage_ctx.dag.downstream(stage_ctx.stage)
      task = stage_ctx.stage.task
      downstream_queues = downstream.filter_map { |ds| task&.find_queue(ds) }
      OutputQueue.new(
        stage_ctx.stage,
        downstream_queues,
        stage_ctx.runtime_edges,
        stage_stats: stage_ctx.stage_stats
      )
    end

    # Consolidated end signal logic used by all stage types
    def send_end_signals(stage_ctx)
      dag_downstream = stage_ctx.dag.downstream(stage_ctx.stage)
      dynamic_targets = stage_ctx.runtime_edges[stage_ctx.stage].to_a
      all_targets = (dag_downstream + dynamic_targets).uniq
      task = stage_ctx.stage.task

      all_targets.each do |target|
        queue = task.find_queue(target)
        next unless queue

        queue << EndOfSource.new(stage_ctx.stage)
      end
    end

    # Call the stage's #call method with appropriate args based on arity
    def call_with_arity(*args, &)
      arity = method(:call).arity.abs
      call(*args[...arity], &)
    end
  end

  # Producer stage - executes once, no input
  class ProducerStage < Stage
    def execute(context, _input_queue, output_queue, _stage_stats)
      if @block
        context.instance_exec(output_queue, &@block)
      elsif respond_to?(:call)
        call_with_arity(output_queue, &output_queue.to_proc)
      end
    end

    def log_type
      'Producer'
    end

    def run_mode
      :autonomous # Generates data independently
    end

    def run_stage(stage_ctx)
      # Execute before hooks
      execute_hooks(stage_ctx, :before)

      # Create output queue
      output_queue = create_output_queue(stage_ctx)

      # Execute producer block directly (ProducerStage doesn't use executor since it's autonomous)
      context = stage_ctx.root_pipeline.context
      execute(context, nil, output_queue, stage_ctx.stage_stats)

      # Execute after hooks
      execute_hooks(stage_ctx, :after)
    ensure
      send_end_signals(stage_ctx)
    end

    private

    def execute_hooks(ctx, type)
      ctx.root_pipeline.execute_stage_hooks(type, ctx.stage)
    end
  end

  # Consumer/Processor stage - loops on input, processes items
  class ConsumerStage < Stage
    def execute(context, input_queue, output_queue, stage_stats)
      # Consumer stages pop from input_queue and process items
      loop do
        item = input_queue.pop

        # Just break from the loop - the worker_loop will handle signaling completion
        break if item.is_a?(EndOfStage)

        # Execute the block or call method with the item, tracking per-item latency
        begin
          start_time = Time.now if stage_stats

          if @block
            context.instance_exec(item, output_queue, &@block)
          elsif respond_to?(:call)
            call_with_arity(item, output_queue, &output_queue.to_proc)
          end

          # Record per-item latency for bottleneck detection
          stage_stats&.record_latency(Time.now - start_time)
        rescue StandardError => e
          # Log item-level errors but continue processing
          Minigun.logger.error "[Stage:#{name}] Error processing item: #{e.message}"
          Minigun.logger.debug e.backtrace.join("\n") if Minigun.logger.debug?
        end
      end
    end

    def run_stage(stage_ctx)
      # Execute before hooks
      stage_ctx.root_pipeline.send(:execute_stage_hooks, :before, stage_ctx.stage)

      # Create wrapped queues
      input_queue = create_input_queue(stage_ctx)
      output_queue = create_output_queue(stage_ctx)

      # Execute via executor (defines HOW: inline/threaded/process)
      context = stage_ctx.root_pipeline.context
      stage_ctx.executor.execute_stage(self, context, input_queue, output_queue)

      # Execute after hooks
      stage_ctx.root_pipeline.send(:execute_stage_hooks, :after, stage_ctx.stage)

      # Flush and cleanup
      flush_if_needed(stage_ctx, output_queue)
    ensure
      send_end_signals(stage_ctx)
    end

    private

    def flush_if_needed(stage_ctx, output_queue)
      return unless respond_to?(:flush)

      context = stage_ctx.root_pipeline.context
      flush(context, output_queue)
    end
  end

  # Accumulator stage - batches items before passing to consumer
  # Collects N items, then emits them as a batch
  class AccumulatorStage < ConsumerStage
    attr_reader :max_size, :max_wait

    # Positional constructor: AccumulatorStage.new(name, pipeline, block, options)
    def initialize(name, pipeline, block, options = {})
      super(name, pipeline, block, options)

      @max_size = options[:max_size] || 100
      @max_wait = options[:max_wait] || nil # Future: time-based batching
      @buffer = []
      @mutex = Mutex.new
    end

    # Override execute to buffer items and emit batches via output queue
    def execute(context, input_queue, output_queue, stage_stats)
      loop do
        item = input_queue.pop

        break if item.is_a?(EndOfStage)

        buffer = nil

        @mutex.synchronize do
          @buffer << item

          if @buffer.size >= @max_size
            buffer = @buffer.dup
            @buffer.clear
          end
        end

        next unless buffer && output_queue

        begin
          start_time = Time.now if stage_stats

          if @block
            # Accumulator block receives |batch, output| like other stages
            context.instance_exec(buffer, output_queue, &@block)
          else
            # No block - just pass through
            output_queue << buffer
          end

          # Record per-batch latency
          stage_stats&.record_latency(Time.now - start_time)
        rescue StandardError => e
          # Log batch-level errors but continue processing
          Minigun.logger.error "[Stage:#{name}] Error processing batch: #{e.message}"
          Minigun.logger.debug e.backtrace.join("\n") if Minigun.logger.debug?
        end
      end
    end

    # Called at end of pipeline to flush remaining items
    def flush(context, output_queue)
      buffer = nil

      @mutex.synchronize do
        if !@buffer.empty?
          buffer = @buffer.dup
          @buffer.clear
        end
      end

      return unless buffer && output_queue

      if @block
        # Accumulator block receives |batch, output| like other stages
        context.instance_exec(buffer, output_queue, &@block)
      else
        # No block - just pass through
        output_queue << buffer
      end
    end
  end

  # Router stages for fan-out patterns
  # Base functionality for all routers
  class RouterStage < Stage
    attr_accessor :targets

    # Positional constructor: RouterStage.new(name, pipeline, targets, options)
    def initialize(name, pipeline, targets, options = {})
      super(name, pipeline, nil, options)
      @targets = targets || []
    end

    protected

    def send_end_signals(worker_ctx)
      # Broadcast EndOfSource to ALL router targets
      task = worker_ctx.stage.task
      @targets.each do |target|
        queue = task&.find_queue(target)
        queue&.<< EndOfSource.new(worker_ctx.stage)
      end
    end
  end

  # Broadcast router - sends each item to ALL downstream stages
  class RouterBroadcastStage < RouterStage
    def run_stage(worker_ctx)
      task = worker_ctx.stage.task

      loop do
        item = worker_ctx.input_queue.pop

        if item.is_a?(EndOfSource)
          worker_ctx.sources_expected << item.stage
          worker_ctx.sources_done << item.stage
          break if worker_ctx.sources_done == worker_ctx.sources_expected

          next
        end

        # Handle routed items from IPC dynamic routing
        if item.is_a?(Minigun::RoutedItem)
          # Route to specific target stage only
          target = @targets.find { |t| t.name == item.target_stage }
          if target
            queue = task&.find_queue(target)
            queue&.<< item.item
          else
            Minigun.logger.warn "[RouterBroadcast] Unknown routed target: #{item.target_stage}"
          end
          next
        end

        # Broadcast to all downstream stages (fan-out semantics)
        @targets.each do |target|
          queue = task&.find_queue(target)
          queue&.<< item
        end
      end
    ensure
      send_end_signals(worker_ctx)
    end
  end

  # Round-robin router - distributes items across downstream stages
  class RouterRoundRobinStage < RouterStage
    def run_stage(worker_ctx)
      task = worker_ctx.stage.task
      target_queues = @targets.map { |target| task&.find_queue(target) }.compact
      round_robin_index = 0

      loop do
        item = worker_ctx.input_queue.pop

        if item.is_a?(EndOfSource)
          worker_ctx.sources_expected << item.stage
          worker_ctx.sources_done << item.stage
          break if worker_ctx.sources_done == worker_ctx.sources_expected

          next
        end

        # Handle routed items from IPC dynamic routing
        if item.is_a?(Minigun::RoutedItem)
          # Route to specific target stage only
          target = @targets.find { |t| t.name == item.target_stage }
          if target
            queue = task&.find_queue(target)
            queue&.<< item.item
          else
            Minigun.logger.warn "[RouterRoundRobin] Unknown routed target: #{item.target_stage}"
          end
          next
        end

        # Round-robin to downstream stages
        target_queues[round_robin_index] << item
        round_robin_index = (round_robin_index + 1) % target_queues.size
      end
    ensure
      send_end_signals(worker_ctx)
    end
  end

  # Special exit stage for nested pipelines
  # Automatically created when a pipeline has output to parent
  class ExitStage < ConsumerStage
    # Positional constructor: ExitStage.new(name, pipeline, block, options)
    def initialize(name, pipeline, block, options = {})
      super(name, pipeline, block, options)
    end
  end

  # Stage that wraps and executes a nested pipeline
  class PipelineStage < Stage
    attr_reader :nested_pipeline

    # Positional constructor: PipelineStage.new(name, pipeline, nested_pipeline, options)
    def initialize(name, pipeline, nested_pipeline, options = {})
      super(name, pipeline, nil, options)
      @nested_pipeline = nested_pipeline
    end


    def run_mode
      :composite # Manages internal stages
    end

    # Run the nested pipeline when this stage is executed as a worker
    def run_stage(stage_ctx)
      return unless @nested_pipeline

      # Set up input/output queues for the nested pipeline
      # Pass the PipelineStage's input queue to nested pipeline so entry stages can use it
      if !stage_ctx.sources_expected.empty?
        # Has upstream: pass input queue to nested pipeline
        # The nested pipeline will handle distributing to its entry stages
        @nested_pipeline.instance_variable_set(:@input_queues, {
          input: stage_ctx.input_queue,
          sources_expected: stage_ctx.sources_expected
        })
      end

      # Always set output queue so pipeline creates :_exit
      @nested_pipeline.instance_variable_set(:@output_queues, { output: create_output_queue(stage_ctx) })

      # Run the nested pipeline (it will handle input distribution to entry stages)
      @nested_pipeline.run(stage_ctx.root_pipeline.context)
    ensure
      send_end_signals(stage_ctx)
    end
  end
end
